{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation\n",
    "We assume the generator is composed of $k$ parts, each of which is generated by a diffeomorphic function $g_i:[0,1]^l \\mapsto \\mathbb{R}^m$. The final observation is simply the stacking of the individual parts, i.e., $g:[0,1)^{k\\times l} \\mapsto \\mathbb{R}^{k\\times m}$ with $g({\\bf z}_1,{\\bf z}_2,{\\bf z}_3)=[g_1({\\bf z}_1), g_2({\\bf z}_2), g_3({\\bf z}_3)]$.\n",
    "\n",
    "We consider two different sampling strategies from the latent space:\n",
    "\n",
    "1. **Random**: Sample uniformly from the full latent distribution $[0,1)^{k \\times l}$.\n",
    "2. **Diagonal**: Sample $\\bf v$ uniformly from $[0,1)^l$ and then generate samples according to $g({\\bf v},{\\bf v},{\\bf v})$.\n",
    "\n",
    "We follow [1,2,3] and design the mixing function as an MLP with\n",
    "- 2 layers (hidden layer dimension $D$)\n",
    "- leaky ReLU (with 0.2 negative slope) to ensure invertability\n",
    "- $L_2$-normalized weight matrices with minimum condition number of 7500 uniformely distributed samples\n",
    "- same number of units in all layers?\n",
    "- what about bias?\n",
    "\n",
    "---\n",
    "- [1]: A. Hyvarinen and H. Morioka, “Nonlinear ICA of Temporally Dependent Stationary Sources,” in Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, Apr. 2017, pp. 460–469. Accessed: Jul. 06, 2022. [Online]. Available: https://proceedings.mlr.press/v54/hyvarinen17a.html\n",
    "- [2] A. Hyvarinen and H. Morioka, “Unsupervised Feature Extraction by Time-Contrastive Learning and Nonlinear ICA,” in Advances in Neural Information Processing Systems, 2016, vol. 29. Accessed: Jul. 06, 2022. [Online]. Available: https://proceedings.neurips.cc/paper/2016/hash/d305281faf947ca7acade9ad5c8c818c-Abstract.html\n",
    "- [3] J. Brady et al., \"Provably Learning Object-Centric Representations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def get_generators(k: int, l: int, m: int, D: int=50) -> list[torch.nn.Module]:\n",
    "    return [build_generator(l, m, D) for _ in range(k)]\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_regression_targets(n:int, k: int, l: int, generators: list[torch.nn.Module], sample_mode: str='random') -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    if sample_mode == 'random':\n",
    "        z = torch.rand(n, k, l)\n",
    "    elif sample_mode == 'diagonal':\n",
    "        z = torch.repeat_interleave(torch.rand(n, l), k, dim=0)\n",
    "        z = torch.reshape(z, (n, k, l))\n",
    "    \n",
    "    x = [torch.stack([generators[j](z[i][j]) for j in range(k)]) for i in range(n)]\n",
    "    x = torch.stack(x)\n",
    "\n",
    "    return z, x\n",
    "\n",
    "\n",
    "def build_generator(l: int, m: int, D: int, slope: float=0.2) -> nn.Sequential:\n",
    "    g = nn.Sequential(\n",
    "        nn.Linear(l, D),\n",
    "        nn.LeakyReLU(slope),\n",
    "        nn.Linear(D, m),\n",
    "        nn.LeakyReLU(slope)\n",
    "    )\n",
    "    g.apply(init_min_cond)\n",
    "    return g\n",
    "\n",
    "\n",
    "# class Generator(torch.nn.Module):\n",
    "#     def __init__(self, l: int, m: int, D: int):\n",
    "#         super(Generator, self).__init__()\n",
    "#         self.fc1 = nn.Linear(l, D)\n",
    "#         self.relu1 = nn.LeakyReLU(0.2)\n",
    "#         self.fc2 = nn.Linear(D, m)\n",
    "#         self.relu2 = nn.LeakyReLU(0.2)\n",
    "#         self.apply(init_min_cond)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = self.relu1(self.fc1(x))\n",
    "#         x = self.relu2(self.fc2(x))\n",
    "#         return x\n",
    "\n",
    "\n",
    "def init_min_cond(m: torch.nn.Module, n_samples: int=7500) -> torch.Tensor:\n",
    "    if isinstance(m, nn.Linear):\n",
    "        w = m.weight.data\n",
    "        k = 1 / w.size(0)\n",
    "\n",
    "        w = torch.nn.functional.normalize(w, p=2)\n",
    "        cond = condition_number(w)\n",
    "\n",
    "        for _ in range(n_samples):\n",
    "            _w = 2 * math.sqrt(k) * torch.rand(w.size()) - math.sqrt(k)\n",
    "            _w = nn.functional.normalize(_w, p=2)\n",
    "            _cond = condition_number(_w)\n",
    "\n",
    "            if _cond < cond:\n",
    "                w = _w\n",
    "                cond = _cond\n",
    "        \n",
    "        m.weight.data = w\n",
    "\n",
    "\n",
    "def condition_number(t: torch.Tensor) -> float:\n",
    "    return torch.norm(t, p=2) / torch.norm(torch.pinverse(t), p=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "We consider two feedforward models:\n",
    "1. An appropriately sized MLP that maps the full $\\mathbb{R}^{k \\times m}$ to the full latent space $\\mathbb [0, 1)^{k\\times l}$.\n",
    "2. A “compositional” model consisting of $k$ MLPs $f_i({\\bf x}): \\mathbb{R}^{k \\times m}\\mapsto [0,1)^{l}$ that each map to a subpart of the latents. Most importantly, the first MLP $f_1$ receives $[g_1({\\bf z}_1),{\\bf 0},{\\bf 0}]$ as an input, the second MLP receives $[{\\bf 0},g_2({\\bf z}_2),{\\bf 0}]$, and so forth. Doing that ensures that the model is compositional by design, but the input dimension is as close as possible to 1 (to avoid confounders).\n",
    "\n",
    "We follow [1] and design the model with\n",
    "- 2 layers\n",
    "- hidden layer of size $D = 120$ for the MLP and $D_i = \\frac{D}{k}$ for the models in the compositional MLP to roughly match the number of parameters\n",
    "- LeakyReLU with slope 0.2\n",
    "\n",
    "---\n",
    "[1] J. Brady et al., \"Provably Learning Object-Centric Representations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_MLP(d_in: int, d_out: int, D: int=120, slope: float=0.2) -> nn.Sequential:\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(d_in, D),\n",
    "        nn.LeakyReLU(slope),\n",
    "        nn.Linear(D, d_out),\n",
    "        nn.LeakyReLU(slope)\n",
    "    )\n",
    "\n",
    "\n",
    "def MLP(k: int, l: int, m: int, D: int=120):\n",
    "    return build_MLP(k * m, k * l, D)\n",
    "\n",
    "\n",
    "class CompositionalMLP(torch.nn.Module):\n",
    "    def __init__(self, k: int, l: int, m: int, D: int=120):\n",
    "        super(CompositionalMLP, self).__init__()\n",
    "        self.k = k\n",
    "        self.models = nn.ModuleList([build_MLP(k * m, l, round(D / k)) for _ in range(k)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.size(0), self.k, -1)\n",
    "        out = []\n",
    "        for i in range(len(self.models)):\n",
    "            x_i = torch.zeros_like(x)\n",
    "            x_i[:, i, :] = x[:, i, :]\n",
    "            x_i = torch.flatten(x_i, start_dim = 1)\n",
    "            out.append(self.models[i](x_i))\n",
    "        return torch.cat(out, dim=1)\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Test\n",
    "We do simple supervised regression and evaluate the $R^2$ distance on random samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchmetrics import R2Score\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, n: int, k: int, l: int, generators: list[torch.nn.Module], sample_mode: str='random'):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.n = n\n",
    "        self.z, self.x = get_regression_targets(n, k, l, generators, sample_mode)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.z[idx]\n",
    "\n",
    "\n",
    "def train(model: torch.nn.Module, trainloader: torch.utils.data.DataLoader, lr: float=0.001, epochs: int=10):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    # for epoch in tqdm(range(epochs)):\n",
    "    for epoch in range(epochs):\n",
    "        cum_loss = 0\n",
    "\n",
    "        for batch, data in enumerate(trainloader, 0):\n",
    "            x, z = data\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            out = model(torch.flatten(x, start_dim=1))\n",
    "            loss = criterion(out, torch.flatten(z, start_dim=1))\n",
    "            cum_loss += loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        cum_loss /= (batch + 1)\n",
    "    \n",
    "    return cum_loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model: torch.nn.Module, testloader: torch.utils.data.DataLoader):\n",
    "    cum_score = 0\n",
    "\n",
    "    for batch, data in enumerate(testloader, 0):\n",
    "        x, z = data\n",
    "        out = model(torch.flatten(x, start_dim=1))\n",
    "        r2score = R2Score(out.size(1))\n",
    "        score = r2score(out, torch.flatten(z, start_dim=1))\n",
    "        cum_score += score\n",
    "    \n",
    "    cum_score /= (batch + 1)\n",
    "    return cum_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build generators...\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "k = 4\n",
    "l = 2\n",
    "m = 10\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "print('Build generators...')\n",
    "g = get_generators(k, l, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build test data...\n"
     ]
    }
   ],
   "source": [
    "print('Build test data...')\n",
    "te_ds = Dataset(1000, k, l, g, 'random')\n",
    "te_ldr = torch.utils.data.DataLoader(te_ds, batch_size=1000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build train data...\n",
      "Build models...\n",
      "Train models...\n",
      "    0\tloss: 0.0556\tR²: 0.3451\n",
      "    1\tloss: 0.0398\tR²: 0.5313\n",
      "    2\tloss: 0.0285\tR²: 0.6628\n",
      "    3\tloss: 0.0212\tR²: 0.7460\n",
      "    4\tloss: 0.0167\tR²: 0.7986\n",
      "    5\tloss: 0.0137\tR²: 0.8341\n",
      "    6\tloss: 0.0112\tR²: 0.8629\n",
      "    7\tloss: 0.0091\tR²: 0.8877\n",
      "    8\tloss: 0.0073\tR²: 0.9091\n",
      "    9\tloss: 0.0059\tR²: 0.9254\n",
      "   10\tloss: 0.0049\tR²: 0.9381\n",
      "   11\tloss: 0.0042\tR²: 0.9470\n",
      "   12\tloss: 0.0037\tR²: 0.9530\n",
      "   13\tloss: 0.0033\tR²: 0.9581\n",
      "   14\tloss: 0.0031\tR²: 0.9611\n",
      "   15\tloss: 0.0029\tR²: 0.9637\n",
      "   16\tloss: 0.0028\tR²: 0.9652\n",
      "   17\tloss: 0.0027\tR²: 0.9670\n",
      "   18\tloss: 0.0026\tR²: 0.9680\n",
      "   19\tloss: 0.0025\tR²: 0.9689\n",
      "   20\tloss: 0.0024\tR²: 0.9699\n",
      "   21\tloss: 0.0024\tR²: 0.9709\n",
      "   22\tloss: 0.0023\tR²: 0.9716\n",
      "   23\tloss: 0.0022\tR²: 0.9725\n",
      "   24\tloss: 0.0021\tR²: 0.9735\n",
      "   25\tloss: 0.0020\tR²: 0.9746\n",
      "   26\tloss: 0.0020\tR²: 0.9753\n",
      "   27\tloss: 0.0019\tR²: 0.9759\n",
      "   28\tloss: 0.0019\tR²: 0.9765\n",
      "   29\tloss: 0.0018\tR²: 0.9768\n",
      "   30\tloss: 0.0018\tR²: 0.9772\n",
      "   31\tloss: 0.0018\tR²: 0.9780\n",
      "   32\tloss: 0.0017\tR²: 0.9785\n",
      "   33\tloss: 0.0017\tR²: 0.9789\n",
      "   34\tloss: 0.0017\tR²: 0.9790\n",
      "   35\tloss: 0.0016\tR²: 0.9797\n",
      "   36\tloss: 0.0016\tR²: 0.9798\n",
      "   37\tloss: 0.0016\tR²: 0.9804\n",
      "   38\tloss: 0.0015\tR²: 0.9807\n",
      "   39\tloss: 0.0015\tR²: 0.9810\n",
      "   40\tloss: 0.0015\tR²: 0.9815\n",
      "   41\tloss: 0.0014\tR²: 0.9818\n",
      "   42\tloss: 0.0014\tR²: 0.9821\n",
      "   43\tloss: 0.0014\tR²: 0.9824\n",
      "   44\tloss: 0.0014\tR²: 0.9827\n",
      "   45\tloss: 0.0013\tR²: 0.9831\n",
      "   46\tloss: 0.0013\tR²: 0.9833\n",
      "   47\tloss: 0.0013\tR²: 0.9835\n",
      "   48\tloss: 0.0013\tR²: 0.9836\n",
      "   49\tloss: 0.0013\tR²: 0.9839\n"
     ]
    }
   ],
   "source": [
    "n = 1000\n",
    "bs = 4\n",
    "e = 100\n",
    "\n",
    "print('Build train data...')\n",
    "tr_ds_rand = Dataset(n, k, l, g, 'random')\n",
    "tr_ds_diag = Dataset(n, k, l, g, 'diagonal')\n",
    "tr_ldr_rand = torch.utils.data.DataLoader(tr_ds_rand, batch_size=bs, shuffle=True)\n",
    "tr_ldr_diag = torch.utils.data.DataLoader(tr_ds_diag, batch_size=bs, shuffle=True)\n",
    "\n",
    "print('Build models...')\n",
    "mlp_rand = MLP(k, l, m)\n",
    "mlp_diag = copy.deepcopy(mlp_rand)\n",
    "cmlp_rand = CompositionalMLP(k, l, m)\n",
    "cmlp_diag = copy.deepcopy(cmlp_rand)\n",
    "\n",
    "print('Train models...')\n",
    "for i in range(50):\n",
    "    loss = train(cmlp_rand, tr_ldr_rand, epochs=10)\n",
    "    score = test(cmlp_rand, te_ldr)\n",
    "    print(f'  {i:3d}\\tloss: {loss:2.4f}\\tR²: {score:2.4f}')\n",
    "    # train(mlp_diag, tr_ldr_diag, epochs=e)\n",
    "    # train(cmlp_rand, tr_ldr_rand, epochs=e)\n",
    "    # train(cmlp_diag, tr_ldr_diag, epochs=e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
