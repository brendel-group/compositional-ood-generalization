{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation\n",
    "We assume the generator is composed of $k$ parts, each of which is generated by a diffeomorphic function $g_i:[0,1]^l \\mapsto \\mathbb{R}^m$. The final observation is simply the stacking of the individual parts, i.e., $g:[0,1)^{k\\times l} \\mapsto \\mathbb{R}^{k\\times m}$ with $g({\\bf z}_1,{\\bf z}_2,{\\bf z}_3)=[g_1({\\bf z}_1), g_2({\\bf z}_2), g_3({\\bf z}_3)]$.\n",
    "\n",
    "We consider two different sampling strategies from the latent space:\n",
    "\n",
    "1. **Random**: Sample uniformly from the full latent distribution $[0,1)^{k \\times l}$.\n",
    "2. **Diagonal**: Sample $\\bf v$ uniformly from $[0,1)^l$ and then generate samples according to $g({\\bf v},{\\bf v},{\\bf v})$.\n",
    "\n",
    "We follow [1,2,3] and design the mixing function as an MLP with\n",
    "- 2 layers (hidden layer dimension $D$)\n",
    "- leaky ReLU (with 0.2 negative slope) to ensure invertability\n",
    "- $L_2$-normalized weight matrices with minimum condition number of 7500 uniformely distributed samples\n",
    "- same number of units in all layers?\n",
    "- what about bias?\n",
    "\n",
    "---\n",
    "- [1]: A. Hyvarinen and H. Morioka, “Nonlinear ICA of Temporally Dependent Stationary Sources,” in Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, Apr. 2017, pp. 460–469. Accessed: Jul. 06, 2022. [Online]. Available: https://proceedings.mlr.press/v54/hyvarinen17a.html\n",
    "- [2] A. Hyvarinen and H. Morioka, “Unsupervised Feature Extraction by Time-Contrastive Learning and Nonlinear ICA,” in Advances in Neural Information Processing Systems, 2016, vol. 29. Accessed: Jul. 06, 2022. [Online]. Available: https://proceedings.neurips.cc/paper/2016/hash/d305281faf947ca7acade9ad5c8c818c-Abstract.html\n",
    "- [3] J. Brady et al., \"Provably Learning Object-Centric Representations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "def get_generators(k: int, l: int, m: int, D: int=50) -> list[torch.nn.Module]:\n",
    "    return [build_generator(l, m, D) for _ in range(k)]\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_regression_targets(n:int, k: int, l: int, generators: list[torch.nn.Module], sample_mode: str='random') -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    if sample_mode == 'random':\n",
    "        z = torch.rand(n, k, l)\n",
    "    elif sample_mode == 'diagonal':\n",
    "        z = torch.repeat_interleave(torch.rand(n, l), k, dim=0)\n",
    "        z = torch.reshape(z, (n, k, l))\n",
    "    \n",
    "    x = [torch.stack([generators[j](z[i][j]) for j in range(k)]) for i in range(n)]\n",
    "    x = torch.stack(x)\n",
    "\n",
    "    return z, x\n",
    "\n",
    "\n",
    "def build_generator(l: int, m: int, D: int, slope: float=0.2) -> nn.Sequential:\n",
    "    g = nn.Sequential(\n",
    "        nn.Linear(l, D),\n",
    "        nn.LeakyReLU(slope),\n",
    "        nn.Linear(D, m),\n",
    "        nn.LeakyReLU(slope)\n",
    "    )\n",
    "    g.apply(init_min_cond)\n",
    "    return g\n",
    "\n",
    "\n",
    "# class Generator(torch.nn.Module):\n",
    "#     def __init__(self, l: int, m: int, D: int):\n",
    "#         super(Generator, self).__init__()\n",
    "#         self.fc1 = nn.Linear(l, D)\n",
    "#         self.relu1 = nn.LeakyReLU(0.2)\n",
    "#         self.fc2 = nn.Linear(D, m)\n",
    "#         self.relu2 = nn.LeakyReLU(0.2)\n",
    "#         self.apply(init_min_cond)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = self.relu1(self.fc1(x))\n",
    "#         x = self.relu2(self.fc2(x))\n",
    "#         return x\n",
    "\n",
    "\n",
    "def init_min_cond(m: torch.nn.Module, n_samples: int=7500) -> torch.Tensor:\n",
    "    if isinstance(m, nn.Linear):\n",
    "        w = m.weight.data\n",
    "        k = 1 / w.size(0)\n",
    "\n",
    "        w = torch.nn.functional.normalize(w, p=2)\n",
    "        cond = condition_number(w)\n",
    "\n",
    "        for _ in range(n_samples):\n",
    "            _w = 2 * math.sqrt(k) * torch.rand(w.size()) - math.sqrt(k)\n",
    "            _w = nn.functional.normalize(_w, p=2)\n",
    "            _cond = condition_number(_w)\n",
    "\n",
    "            if _cond < cond:\n",
    "                w = _w\n",
    "                cond = _cond\n",
    "        \n",
    "        m.weight.data = w\n",
    "\n",
    "\n",
    "def condition_number(t: torch.Tensor) -> float:\n",
    "    return torch.norm(t, p=2) / torch.norm(torch.pinverse(t), p=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = get_generators(4, 2, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "We consider two feedforward models:\n",
    "1. An appropriately sized MLP that maps the full $\\mathbb{R}^{k \\times m}$ to the full latent space $\\mathbb [0, 1)^{k\\times l}$.\n",
    "2. A “compositional” model consisting of $k$ MLPs $f_i({\\bf x}): \\mathbb{R}^{k \\times m}\\mapsto [0,1)^{l}$ that each map to a subpart of the latents. Most importantly, the first MLP $f_1$ receives $[g_1({\\bf z}_1),{\\bf 0},{\\bf 0}]$ as an input, the second MLP receives $[{\\bf 0},g_2({\\bf z}_2),{\\bf 0}]$, and so forth. Doing that ensures that the model is compositional by design, but the input dimension is as close as possible to 1 (to avoid confounders).\n",
    "\n",
    "We follow [1] and design the model with\n",
    "- 2 layers\n",
    "- hidden layer of size $D = 120$ for the MLP and $D_i = \\frac{D}{k}$ for the models in the compositional MLP to roughly match the number of parameters\n",
    "- LeakyReLU with slope 0.2\n",
    "\n",
    "---\n",
    "[1] J. Brady et al., \"Provably Learning Object-Centric Representations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_MLP(d_in: int, d_out: int, D: int=120, slope: float=0.2) -> nn.Sequential:\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(d_in, D),\n",
    "        nn.LeakyReLU(slope),\n",
    "        nn.Linear(D, d_out),\n",
    "        nn.LeakyReLU(slope)\n",
    "    )\n",
    "\n",
    "\n",
    "def MLP(k: int, l: int, m: int, D: int=120):\n",
    "    return build_MLP(k * m, k * l, D)\n",
    "\n",
    "\n",
    "class CompositionalMLP(torch.nn.Module):\n",
    "    def __init__(self, k: int, l: int, m: int, D: int=120):\n",
    "        super(CompositionalMLP, self).__init__()\n",
    "        self.models = [build_MLP(k * m, l, round(D / k)) for _ in range(k)]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = []\n",
    "        for i in range(len(self.models)):\n",
    "            x_i = torch.zeros_like(x)\n",
    "            x_i[:, i, :] = x[:, i, :]\n",
    "            x_i = torch.flatten(x_i, start_dim = 1)\n",
    "            out.append(self.models[i](x_i))\n",
    "        return torch.cat(out)\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(4, 2, 10)\n",
    "cmlp = CompositionalMLP(4, 2, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "z, x = get_regression_targets(100, 4, 2, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
