{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thadd/anaconda3/envs/ml/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    dev = 'cuda:0'\n",
    "else:\n",
    "    dev = 'cpu'\n",
    "device = torch.device(dev)\n",
    "\n",
    "def get_generators(k: int, l: int, m: int, D: int=50) -> list[torch.nn.Module]:\n",
    "    return [build_generator(l, m, D) for _ in range(k)]\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_regression_targets(n:int, k: int, l: int, generators: list[torch.nn.Module], sample_mode: str='random') -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    if sample_mode == 'random':\n",
    "        z = torch.rand(n, k, l)\n",
    "    elif sample_mode == 'diagonal':\n",
    "        z = torch.repeat_interleave(torch.rand(n, l), k, dim=0)\n",
    "        z = torch.reshape(z, (n, k, l))\n",
    "    elif sample_mode == 'orthogonal':\n",
    "        _z = torch.rand(n, l)\n",
    "        mask = torch.stack([torch.arange(n), torch.randint(k, (n, 1)).squeeze(dim=1)], dim=1).long()\n",
    "        z = torch.zeros(n, k, l)\n",
    "        z[mask.chunk(chunks=2, dim=1)] = _z.unsqueeze(1)\n",
    "    \n",
    "    x = [torch.stack([generators[j](z[i][j]) for j in range(k)]) for i in range(n)]\n",
    "    x = torch.stack(x)\n",
    "\n",
    "    return z, x\n",
    "\n",
    "def build_generator(l: int, m: int, D: int, slope: float=0.2) -> nn.Sequential:\n",
    "    g = nn.Sequential(\n",
    "        nn.Linear(l, D),\n",
    "        nn.LeakyReLU(slope),\n",
    "        nn.Linear(D, m),\n",
    "        nn.LeakyReLU(slope)\n",
    "    )\n",
    "    g.apply(init_min_cond)\n",
    "    return g\n",
    "\n",
    "\n",
    "# class Generator(torch.nn.Module):\n",
    "#     def __init__(self, l: int, m: int, D: int):\n",
    "#         super(Generator, self).__init__()\n",
    "#         self.fc1 = nn.Linear(l, D)\n",
    "#         self.relu1 = nn.LeakyReLU(0.2)\n",
    "#         self.fc2 = nn.Linear(D, m)\n",
    "#         self.relu2 = nn.LeakyReLU(0.2)\n",
    "#         self.apply(init_min_cond)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = self.relu1(self.fc1(x))\n",
    "#         x = self.relu2(self.fc2(x))\n",
    "#         return x\n",
    "\n",
    "\n",
    "def init_min_cond(m: torch.nn.Module, n_samples: int=7500) -> torch.Tensor:\n",
    "    if isinstance(m, nn.Linear):\n",
    "        w = m.weight.data\n",
    "        k = 1 / w.size(0)\n",
    "\n",
    "        w = torch.nn.functional.normalize(w, p=2)\n",
    "        cond = condition_number(w)\n",
    "\n",
    "        for _ in range(n_samples):\n",
    "            _w = 2 * math.sqrt(k) * torch.rand(w.size()) - math.sqrt(k)\n",
    "            _w = nn.functional.normalize(_w, p=2)\n",
    "            _cond = condition_number(_w)\n",
    "\n",
    "            if _cond < cond:\n",
    "                w = _w\n",
    "                cond = _cond\n",
    "        \n",
    "        m.weight.data = w\n",
    "\n",
    "\n",
    "def condition_number(t: torch.Tensor) -> float:\n",
    "    return torch.norm(t, p=2) / torch.norm(torch.pinverse(t), p=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "We consider 2 models:\n",
    "- an autoencoder, where we can directly impose regularizations on the decoder\n",
    "- an MLP, where we can only impose regularization on the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_MLP(d_in: int, d_out: int, D: int=120, slope: float=0.2, **kwargs) -> nn.Sequential:\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(d_in, D),\n",
    "        nn.LeakyReLU(slope),\n",
    "        nn.Linear(D, d_out),\n",
    "        nn.LeakyReLU(slope)\n",
    "    )\n",
    "\n",
    "\n",
    "def MLP(k: int, l: int, m: int, D: int=120, **kwargs):\n",
    "    return build_MLP(k * m, k * l, D, **kwargs)\n",
    "\n",
    "\n",
    "def MLP3(k: int, l: int, m: int, D: int=120, slope: float=0.2, **kwargs):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(k * m, D),\n",
    "        nn.LeakyReLU(slope),\n",
    "        nn.Linear(D, D),\n",
    "        nn.LeakyReLU(slope),\n",
    "        nn.Linear(D, k * l),\n",
    "        nn.LeakyReLU(slope)\n",
    "    )\n",
    "\n",
    "\n",
    "class CompositionalMLP(torch.nn.Module):\n",
    "    def __init__(self, k: int, l: int, m: int, D: int=120, **kwargs):\n",
    "        super(CompositionalMLP, self).__init__()\n",
    "        self.k = k\n",
    "        self.models = nn.ModuleList([build_MLP(k * m, l, round(D / k), **kwargs) for _ in range(k)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.size(0), self.k, -1)\n",
    "        out = []\n",
    "        for i in range(len(self.models)):\n",
    "            x_i = torch.zeros_like(x)\n",
    "            x_i[:, i, :] = x[:, i, :]\n",
    "            x_i = torch.flatten(x_i, start_dim = 1)\n",
    "            out.append(self.models[i](x_i))\n",
    "        return torch.cat(out, dim=1)\n",
    "\n",
    "\n",
    "class Autoencoder(torch.nn.Module):\n",
    "    def __init__(self, k: int, l: int, m: int, D: int=120, **kwargs):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.f = build_MLP(k * m, k * l, D, **kwargs)\n",
    "        self.g = build_MLP(k * l, k * m, D, **kwargs)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.f(x)\n",
    "        out = self.g(z)\n",
    "        return out, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "2 regularizations\n",
    "- compositional contrast from Provably Learning Object-Centric Representations#training a model\n",
    "- regularize Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchmetrics import R2Score\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# could be more efficient with torch.utils.data.TensorDataset, but I couldn't be assed to look up the documentation\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, n: int, k: int, l: int, generators: list[torch.nn.Module], sample_mode: str='random'):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.n = n\n",
    "        self.z, self.x = get_regression_targets(n, k, l, generators, sample_mode)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.z[idx]\n",
    "\n",
    "\n",
    "def train(model: torch.nn.Module, trainloader: torch.utils.data.DataLoader, lr: float=0.001, epochs: int=10):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    # for epoch in tqdm(range(epochs)):\n",
    "    for epoch in range(epochs):\n",
    "        cum_loss = 0\n",
    "\n",
    "        for batch, data in enumerate(trainloader, 0):\n",
    "            x, z = data\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            out = model(torch.flatten(x, start_dim=1))\n",
    "            loss = criterion(out, torch.flatten(z, start_dim=1))\n",
    "            cum_loss += loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        cum_loss /= (batch + 1)\n",
    "    \n",
    "    return cum_loss\n",
    "\n",
    "\n",
    "# train for the same number of iterations (batches) independent of dataset size (i.e. without epochs)\n",
    "def train_iter(model: torch.nn.Module, trainloader: torch.utils.data.DataLoader, lr: float=0.001):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    cum_loss = 0\n",
    "\n",
    "    for batch, data in enumerate(trainloader, 0):\n",
    "        x, z = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = model(torch.flatten(x.to(dev), start_dim=1))\n",
    "        loss = criterion(out, torch.flatten(z.to(dev), start_dim=1))\n",
    "        cum_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    cum_loss /= (batch + 1)\n",
    "    return cum_loss.to(torch.device('cpu')).item()\n",
    "\n",
    "\n",
    "# train for the same number of iterations (batches) independent of dataset size (i.e. without epochs)\n",
    "def train_iter_reg(model: torch.nn.Module, trainloader: torch.utils.data.DataLoader, regularization, lamda: float=0.5, lr: float=0.001):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    cum_loss = 0\n",
    "\n",
    "    for batch, data in enumerate(trainloader, 0):\n",
    "        x, z = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x.requires_grad = True\n",
    "        x = x.flatten(1).to(dev)\n",
    "        z = z.flatten(1).to(dev)\n",
    "\n",
    "        z_hat = model(x)\n",
    "        loss = criterion(z_hat, z) + lamda * regularization(model, x)\n",
    "        cum_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    cum_loss /= (batch + 1)\n",
    "    return cum_loss.to(torch.device('cpu')).item()\n",
    "\n",
    "\n",
    "# class CompContrast(torch.nn.Module):\n",
    "#     def __init__(self) -> None:\n",
    "#         super(CompContrast, self).__init__()\n",
    "    \n",
    "#     def forward(self, output, input):\n",
    "#         output.backward(inputs=input)\n",
    "#         return 0\n",
    "\n",
    "\n",
    "def comp_contrast(func: torch.nn.Module, inputs: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Calculate the compositional contrast for a function `func` with respect to `inputs`.\n",
    "\n",
    "    The output is calculated as the mean over the batch dimension.\n",
    "    `inputs` needs to be flattened except for the batch dimension and `requires_grad` needs to be set to `True`.\n",
    "    \"\"\"\n",
    "    assert inputs.requires_grad == True, 'To calculate the derivative by `inputs` `requires_grad` needs to be set to `True`.'\n",
    "\n",
    "    # compute the jacobian with respect to the inputs\n",
    "    # because this is done for the whole batch, the output has dimensions [batch, out, batch, in]\n",
    "    # but Jacobian[i, :, j, :] = 0 because there is no interaction between batches, so dim 2 can be removed\n",
    "    # after indexing the Jacobian has shape [batch, out, in]\n",
    "    jac = torch.autograd.functional.jacobian(func, inputs)\n",
    "    index = torch.arange(jac.shape[0]).reshape(-1, 1, 1, 1).expand(jac.shape[0], jac.shape[1], 1, jac.shape[3]).to(dev)\n",
    "    jac = torch.gather(jac, 2, index).squeeze()\n",
    "\n",
    "    # compute the compositional contrast as the sum of all pairs of partial derivatives for all outputs\n",
    "    # average over the batch dimension\n",
    "    cc = torch.mean(torch.sum(torch.triu(jac.unsqueeze(2).repeat(1, 1, 40, 1) * jac.unsqueeze(2).repeat(1, 1, 40, 1).transpose(2, 3), diagonal=1), dim=(1, 2, 3)))\n",
    "\n",
    "    return cc\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model: torch.nn.Module, testloader: torch.utils.data.DataLoader):\n",
    "    cum_score = 0\n",
    "\n",
    "    for batch, data in enumerate(testloader, 0):\n",
    "        x, z = data\n",
    "        out = model(torch.flatten(x.to(dev), start_dim=1))\n",
    "        r2score = R2Score(out.size(1)).to(dev)\n",
    "        score = r2score(out, torch.flatten(z.to(dev), start_dim=1))\n",
    "        cum_score += score\n",
    "    \n",
    "    cum_score /= (batch + 1)\n",
    "    return cum_score.to(torch.device('cpu')).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build generators...\n",
      "Build test data...\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "k = 4\n",
    "l = 2\n",
    "m = 10\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "print('Build generators...')\n",
    "g = get_generators(k, l, m)\n",
    "\n",
    "print('Build test data...')\n",
    "te_ds = Dataset(1000, k, l, g, 'random')\n",
    "te_ldr = torch.utils.data.DataLoader(te_ds, batch_size=1000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=  16\n",
      "Build train data...\n",
      "Build models...\n",
      "Train models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [09:40<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=  32\n",
      "Build train data...\n",
      "Build models...\n",
      "Train models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [08:42<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=  64\n",
      "Build train data...\n",
      "Build models...\n",
      "Train models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 467/500 [08:09<00:42,  1.29s/it]"
     ]
    }
   ],
   "source": [
    "bs = 64\n",
    "nb = int(2**13 / bs)\n",
    "\n",
    "res = []\n",
    "for log_n in range(4, 13):\n",
    "    n = 2**log_n\n",
    "    print(f'n={n:4d}')\n",
    "\n",
    "    print('Build train data...')\n",
    "    tr_ds_rand = Dataset(n, k, l, g, 'random')\n",
    "    tr_ds_diag = Dataset(n, k, l, g, 'diagonal')\n",
    "    tr_ds_orth = Dataset(n, k, l, g, 'orthogonal')\n",
    "\n",
    "    tr_ldr_rand = torch.utils.data.DataLoader(tr_ds_rand, batch_sampler=torch.utils.data.BatchSampler(torch.utils.data.RandomSampler(tr_ds_rand, num_samples=nb), bs, False))\n",
    "    tr_ldr_diag = torch.utils.data.DataLoader(tr_ds_diag, batch_sampler=torch.utils.data.BatchSampler(torch.utils.data.RandomSampler(tr_ds_diag, num_samples=nb), bs, False))\n",
    "    tr_ldr_orth = torch.utils.data.DataLoader(tr_ds_orth, batch_sampler=torch.utils.data.BatchSampler(torch.utils.data.RandomSampler(tr_ds_orth, num_samples=nb), bs, False))\n",
    "\n",
    "    print('Build models...')\n",
    "    mlp_rand = MLP(k, l, m).to(dev)\n",
    "    mlp_diag = copy.deepcopy(mlp_rand)\n",
    "    mlp_orth = copy.deepcopy(mlp_rand)\n",
    "    # cmlp_rand = CompositionalMLP(k, l, m).to(dev)\n",
    "    # cmlp_diag = copy.deepcopy(cmlp_rand)\n",
    "    # cmlp_orth = copy.deepcopy(cmlp_rand)\n",
    "\n",
    "    print('Train models...')\n",
    "    for i in tqdm(range(500)):\n",
    "        res.append({'metric': 'train loss', 'n samples': n, 'n batches': (i+1)*nb, 'model': 'normal', 'sampling': 'random', 'val': train_iter_reg(mlp_rand, tr_ldr_rand, comp_contrast)})\n",
    "        res.append({'metric': 'test R²', 'n samples': n, 'n batches': (i+1)*nb, 'model': 'normal', 'sampling': 'random', 'val': test(mlp_rand, te_ldr)})\n",
    "        res.append({'metric': 'train loss', 'n samples': n, 'n batches': (i+1)*nb, 'model': 'normal', 'sampling': 'diagonal', 'val': train_iter_reg(mlp_diag, tr_ldr_diag, comp_contrast)})\n",
    "        res.append({'metric': 'test R²', 'n samples': n, 'n batches': (i+1)*nb, 'model': 'normal', 'sampling': 'diagonal', 'val': test(mlp_diag, te_ldr)})\n",
    "        res.append({'metric': 'train loss', 'n samples': n, 'n batches': (i+1)*nb, 'model': 'normal', 'sampling': 'orthogonal', 'val': train_iter_reg(mlp_orth, tr_ldr_orth, comp_contrast)})\n",
    "        res.append({'metric': 'test R²', 'n samples': n, 'n batches': (i+1)*nb, 'model': 'normal', 'sampling': 'orthogonal', 'val': test(mlp_orth, te_ldr)})\n",
    "\n",
    "        # res.append({'metric': 'train loss', 'n samples': n, 'n batches': (i+1)*nb, 'model': 'compositional', 'sampling': 'random', 'val': train_iter(cmlp_rand, tr_ldr_rand)})\n",
    "        # res.append({'metric': 'test R²', 'n samples': n, 'n batches': (i+1)*nb, 'model': 'compositional', 'sampling': 'random', 'val': test(cmlp_rand, te_ldr)})\n",
    "        # res.append({'metric': 'train loss', 'n samples': n, 'n batches': (i+1)*nb, 'model': 'compositional', 'sampling': 'diagonal', 'val': train_iter(cmlp_diag, tr_ldr_diag)})\n",
    "        # res.append({'metric': 'test R²', 'n samples': n, 'n batches': (i+1)*nb, 'model': 'compositional', 'sampling': 'diagonal', 'val': test(cmlp_diag, te_ldr)})\n",
    "        # res.append({'metric': 'train loss', 'n samples': n, 'n batches': (i+1)*nb, 'model': 'compositional', 'sampling': 'orthogonal', 'val': train_iter(cmlp_orth, tr_ldr_orth)})\n",
    "        # res.append({'metric': 'test R²', 'n samples': n, 'n batches': (i+1)*nb, 'model': 'compositional', 'sampling': 'orthogonal', 'val': test(cmlp_orth, te_ldr)})\n",
    "\n",
    "import pandas as pd\n",
    "res_df = pd.DataFrame.from_dict(res)\n",
    "\n",
    "import pickle as pk\n",
    "with open(r'res_i500_comp.pkl', 'wb') as f:\n",
    "    pk.dump(res_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_ds_rand = Dataset(1000, k, l, g, 'random')\n",
    "tr_ldr_rand = torch.utils.data.DataLoader(tr_ds_rand, batch_sampler=torch.utils.data.BatchSampler(torch.utils.data.RandomSampler(tr_ds_rand, num_samples=64), 64, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_rand = MLP3(4, 2, 10).to(dev)\n",
    "\n",
    "x, z = tr_ldr_rand._get_iterator().next()\n",
    "\n",
    "x.requires_grad = True\n",
    "x = x.flatten(1).to(dev)\n",
    "\n",
    "out = mlp_rand(torch.flatten(x.to(dev), start_dim=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = []\n",
    "for batch in range(out.shape[0]):\n",
    "    grads.append([])\n",
    "    for i in range(out.shape[1]):\n",
    "        grads[batch].append(torch.autograd.grad(out[batch, i], x, retain_graph=True, allow_unused=True)[0][batch])\n",
    "grads = torch.stack([torch.stack(_grad) for _grad in grads]).flatten(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = []\n",
    "for b in range(jac.shape[0]):\n",
    "    cc.append([])\n",
    "    for o in range(jac.shape[1]):\n",
    "        _jac = jac[b, o]\n",
    "        cc[b].append(torch.sum(torch.triu(_jac.repeat(40, 1) * _jac.repeat(40, 1).t(), diagonal=1)))\n",
    "cc = torch.stack([torch.stack(_cc) for _cc in cc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the jacobian with respect to the inputs\n",
    "# because this is done for the whole batch, the output has dimensions [batch, out, batch, in]\n",
    "# but Jacobian[i, :, j, :] = 0 because there is no interaction between batches, so dim 2 can be removed\n",
    "# after indexing the Jacobian has shape [batch, out, in]\n",
    "jac = torch.autograd.functional.jacobian(mlp_rand, x)\n",
    "index = torch.arange(jac.shape[0]).reshape(-1, 1, 1, 1).expand(jac.shape[0], jac.shape[1], 1, jac.shape[3]).to(dev)\n",
    "jac = torch.gather(jac, 2, index).squeeze()\n",
    "\n",
    "# compute the compositional contrast as the sum of all pairs of partial derivatives for all outputs\n",
    "# average over the batch dimension\n",
    "# cc = torch.mean(torch.sum(torch.triu(jac.unsqueeze(2).repeat(1, 1, 40, 1) * jac.unsqueeze(2).repeat(1, 1, 40, 1).transpose(2, 3), diagonal=1), dim=(1, 2, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_output(func: callable, idx) -> callable:\n",
    "    def _func(t: torch.Tensor) -> torch.Tensor:\n",
    "        return func(t)[idx]\n",
    "    \n",
    "    return _func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pow_reducer(x):\n",
    "    return x.pow(3).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0130, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_rand(x).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 40])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "hess = torch.autograd.functional.hessian(select_output(mlp_rand, (0, 0)), x)\n",
    "# index = torch.arange(hess.shape[0]).reshape(-1, 1, 1, 1).expand(hess.shape[0], hess.shape[1], 1, hess.shape[3]).to(dev)\n",
    "# hess = torch.gather(hess, 2, index).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hess.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "hessian = []\n",
    "for batch in range(out.shape[0]):\n",
    "    hessian.append([])\n",
    "    for i in range(out.shape[1]):\n",
    "        hessian[batch].append(torch.autograd.functional.hessian(select_output(mlp_rand, (batch, i)), x)[0][batch])\n",
    "hessian = torch.stack([torch.stack(_grad) for _grad in grads]).flatten(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0391aaabe502e8fdd2a9eb75ee06051adc82314891ad66e85fb24892342142e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
